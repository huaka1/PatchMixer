Args in experiment:
Namespace(activation='gelu', affine=0, batch_size=1024, c_out=7, checkpoints='./checkpoints/', d_ff=2048, d_layers=1, d_model=256, data='ETTh1', data_path='ETTh1.csv', dec_in=7, decomposition=0, des='Exp', devices='0,1', distil=True, do_predict=False, dropout=0.2, e_layers=1, embed='timeF', embed_type=0, enc_in=7, factor=1, fc_dropout=0.05, features='M', freq='h', gpu=0, head_dropout=0.0, individual=0, is_training=1, itr=1, kernel_size=25, label_len=0, learning_rate=0.0001, loss='mse', loss_flag=2, lradj='type3', mixer_kernel_size=8, model='PatchMixer', model_id='336_720', moving_avg=25, n_heads=8, num_workers=10, output_attention=False, padding_patch='end', patch_len=16, patience=10, pct_start=0.3, pred_len=720, random_seed=2021, revin=1, root_path='./dataset/ETT-small/', seq_len=336, stride=8, subtract_last=0, target='OT', test_flop=False, train_epochs=100, use_amp=False, use_gpu=True, use_multi_gpu=False)
Use GPU: cuda:0
>>>>>>>start training : loss_flag2_lr0.0001_dm256_336_720_PatchMixer_ETTh1_ftM_sl336_pl720_p16s8_random2021_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 7585
val 2161
test 2161
Epoch: 1 cost time: 4.2809529304504395
Epoch: 1, Steps: 7 | Train Loss: 0.7084934 Vali Loss: 1.3799183 Test Loss: 0.6106940
Validation loss decreased (inf --> 1.379918).  Saving model ...
Updating learning rate to 0.0001
Epoch: 2 cost time: 4.431824445724487
Epoch: 2, Steps: 7 | Train Loss: 0.7133504 Vali Loss: 1.3738544 Test Loss: 0.6495895
Validation loss decreased (1.379918 --> 1.373854).  Saving model ...
Updating learning rate to 0.0001
Epoch: 3 cost time: 4.3881447315216064
Epoch: 3, Steps: 7 | Train Loss: 0.6687705 Vali Loss: 1.2642739 Test Loss: 0.5223225
Validation loss decreased (1.373854 --> 1.264274).  Saving model ...
Updating learning rate to 0.0001
Epoch: 4 cost time: 4.353919506072998
Epoch: 4, Steps: 7 | Train Loss: 0.6401356 Vali Loss: 1.2263795 Test Loss: 0.4841238
Validation loss decreased (1.264274 --> 1.226380).  Saving model ...
Updating learning rate to 9e-05
Epoch: 5 cost time: 4.272794961929321
Epoch: 5, Steps: 7 | Train Loss: 0.6148019 Vali Loss: 1.2073638 Test Loss: 0.4849698
Validation loss decreased (1.226380 --> 1.207364).  Saving model ...
Updating learning rate to 8.1e-05
Epoch: 6 cost time: 4.322717666625977
Epoch: 6, Steps: 7 | Train Loss: 0.6015567 Vali Loss: 1.2026944 Test Loss: 0.4912446
Validation loss decreased (1.207364 --> 1.202694).  Saving model ...
Updating learning rate to 7.290000000000001e-05
Epoch: 7 cost time: 4.313387870788574
Epoch: 7, Steps: 7 | Train Loss: 0.5896303 Vali Loss: 1.1913819 Test Loss: 0.4760298
Validation loss decreased (1.202694 --> 1.191382).  Saving model ...
Updating learning rate to 6.561e-05
Epoch: 8 cost time: 4.386656999588013
Epoch: 8, Steps: 7 | Train Loss: 0.5800550 Vali Loss: 1.1764649 Test Loss: 0.4637902
Validation loss decreased (1.191382 --> 1.176465).  Saving model ...
Updating learning rate to 5.904900000000001e-05
Epoch: 9 cost time: 4.310947418212891
Epoch: 9, Steps: 7 | Train Loss: 0.5744629 Vali Loss: 1.1719147 Test Loss: 0.4581696
Validation loss decreased (1.176465 --> 1.171915).  Saving model ...
Updating learning rate to 5.3144100000000005e-05
Epoch: 10 cost time: 4.354399919509888
Epoch: 10, Steps: 7 | Train Loss: 0.5670717 Vali Loss: 1.1748655 Test Loss: 0.4576107
EarlyStopping counter: 1 out of 10
Updating learning rate to 4.782969000000001e-05
Epoch: 11 cost time: 4.324787855148315
Epoch: 11, Steps: 7 | Train Loss: 0.5638413 Vali Loss: 1.1718844 Test Loss: 0.4568757
Validation loss decreased (1.171915 --> 1.171884).  Saving model ...
Updating learning rate to 4.304672100000001e-05
Epoch: 12 cost time: 4.2656450271606445
Epoch: 12, Steps: 7 | Train Loss: 0.5610480 Vali Loss: 1.1653552 Test Loss: 0.4578162
Validation loss decreased (1.171884 --> 1.165355).  Saving model ...
Updating learning rate to 3.874204890000001e-05
Epoch: 13 cost time: 4.448607921600342
Epoch: 13, Steps: 7 | Train Loss: 0.5581468 Vali Loss: 1.1675978 Test Loss: 0.4576778
EarlyStopping counter: 1 out of 10
Updating learning rate to 3.486784401000001e-05
Epoch: 14 cost time: 4.374792575836182
Epoch: 14, Steps: 7 | Train Loss: 0.5561027 Vali Loss: 1.1723733 Test Loss: 0.4562604
EarlyStopping counter: 2 out of 10
Updating learning rate to 3.138105960900001e-05
Epoch: 15 cost time: 4.316957235336304
Epoch: 15, Steps: 7 | Train Loss: 0.5535080 Vali Loss: 1.1769867 Test Loss: 0.4569564
EarlyStopping counter: 3 out of 10
Updating learning rate to 2.824295364810001e-05
Epoch: 16 cost time: 4.366827964782715
Epoch: 16, Steps: 7 | Train Loss: 0.5514540 Vali Loss: 1.1737758 Test Loss: 0.4578472
EarlyStopping counter: 4 out of 10
Updating learning rate to 2.541865828329001e-05
Epoch: 17 cost time: 4.4072041511535645
Epoch: 17, Steps: 7 | Train Loss: 0.5505721 Vali Loss: 1.1758130 Test Loss: 0.4575924
EarlyStopping counter: 5 out of 10
Updating learning rate to 2.287679245496101e-05
Epoch: 18 cost time: 4.414465665817261
Epoch: 18, Steps: 7 | Train Loss: 0.5487141 Vali Loss: 1.1780479 Test Loss: 0.4575467
EarlyStopping counter: 6 out of 10
Updating learning rate to 2.0589113209464907e-05
Epoch: 19 cost time: 4.337362051010132
Epoch: 19, Steps: 7 | Train Loss: 0.5482268 Vali Loss: 1.1747303 Test Loss: 0.4558928
EarlyStopping counter: 7 out of 10
Updating learning rate to 1.8530201888518416e-05
Epoch: 20 cost time: 4.341238737106323
Epoch: 20, Steps: 7 | Train Loss: 0.5462084 Vali Loss: 1.1794198 Test Loss: 0.4544297
EarlyStopping counter: 8 out of 10
Updating learning rate to 1.6677181699666577e-05
Epoch: 21 cost time: 4.304918527603149
Epoch: 21, Steps: 7 | Train Loss: 0.5447918 Vali Loss: 1.1737769 Test Loss: 0.4562751
EarlyStopping counter: 9 out of 10
Updating learning rate to 1.5009463529699919e-05
Epoch: 22 cost time: 4.286888360977173
Epoch: 22, Steps: 7 | Train Loss: 0.5448344 Vali Loss: 1.1765302 Test Loss: 0.4571992
EarlyStopping counter: 10 out of 10
Early stopping
>>>>>>>testing : loss_flag2_lr0.0001_dm256_336_720_PatchMixer_ETTh1_ftM_sl336_pl720_p16s8_random2021_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<
test 2161
mse:0.45334452390670776, mae:0.4622874855995178, rse:0.6466946601867676
